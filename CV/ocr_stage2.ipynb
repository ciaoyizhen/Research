{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 写在前面\n",
    "TrOCR原论文,预训练了6.8亿的人造数据，算力有限训不动(试过,没大量数据跑不起来)，因此做第二阶段微调。\n",
    "\n",
    "然后huggingface有提供一阶段的参数。但是一阶段效果就很好了。因此这里将使用trocr的印刷体版本训练成手写字版本\n",
    "\n",
    "这里数据好像给的太慢了,导致训练时间长，可以增加num_proc在datasets.map中以及trainingArgument中设置num_work。\n",
    "\n",
    "jupyter在windows中存在多进程bug，所以这里没有实现。在mac和linux环境都是可以直接在jupyter中调用\n",
    "\n",
    "看了很多实现，都没有实现动态padding，这里的代码实现了动态padding。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(\"priyank-m/IAM_words_text_recognition\")\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_raw_dataset = raw_dataset.pop(\"val\")\n",
    "raw_dataset[\"train\"] = concatenate_datasets([raw_dataset[\"train\"], val_raw_dataset])\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = raw_dataset[\"train\"].select(range(10))\n",
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = sample_dataset[0]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = test_data[\"image\"]\n",
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"microsoft/trocr-small-printed\"\n",
    "processor = TrOCRProcessor.from_pretrained(checkpoint)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(checkpoint)\n",
    "\n",
    "# 补一下 好像不在模型自身的config decoder里有\n",
    "model.config.decoder_start_token_id = processor.tokenizer.eos_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_function(examples):\n",
    "    origin_image_list = examples[\"image\"]\n",
    "    origin_text_list = examples[\"text\"]\n",
    "    \n",
    "    image_list = []\n",
    "    text_list = []\n",
    "    for image, text in zip(origin_image_list, origin_text_list):\n",
    "        image = image.convert(\"RGB\")\n",
    "        image_list.append(image)\n",
    "        text_list.append(text)\n",
    "    #BUG processor.tokenizer.bos_token != decoder_start_token 这个放在DataCollator中实现\n",
    "    return processor(images=image_list, text=text_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_sample_dataset = sample_dataset.map(map_function, batched=True)\n",
    "tokenizer_sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from transformers import default_data_collator\n",
    "from typing import Any, List, Dict, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForOCR:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor ([`WhisperProcessor`])\n",
    "            The processor used for processing the data.\n",
    "        decoder_start_token_id (`int`)\n",
    "            The begin-of-sentence of the decoder.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        model_input_name = self.processor.model_input_names[0]\n",
    "        input_features = [{model_input_name: feature[model_input_name]} for feature in features]\n",
    "        # 去掉process.tokenizer.bos_token_id  模型内部会给他补上 decoder_start_token_id 这里补也行 没必要\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"][1:]} for feature in features]\n",
    "\n",
    "        batch = default_data_collator(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collate_fn = DataCollatorForOCR(processor, model.decoder.config.decoder_start_token_id)\n",
    "dataloader = DataLoader(tokenizer_sample_dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "for item in dataloader:\n",
    "    pixel_values = item[\"pixel_values\"]\n",
    "    labels = item[\"labels\"]\n",
    "    print(pixel_values.size())\n",
    "    print(labels.size())\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_datasets = raw_dataset.map(map_function, batched=True, remove_columns=raw_dataset[\"train\"].column_names)\n",
    "tokenizer_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, EvalPrediction\n",
    "\n",
    "wandb.init(project=\"trocr_stage2\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def eval_function(eval_prediction:EvalPrediction):\n",
    "    predictions = eval_prediction.predictions\n",
    "    label_ids = eval_prediction.label_ids\n",
    "    acc = accuracy.compute(references=label_ids, predictions=predictions)\n",
    "    return acc\n",
    "\n",
    "#! 因为动态padding,很有可能训练过程中 爆显存\n",
    "train_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"output/trocr_stage2\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=1000,\n",
    "    num_train_epochs=3,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_safetensors=True,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\"  # 如果不想用wandb 就改成tensorboard\n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=tokenizer_datasets[\"train\"],\n",
    "    eval_dataset=tokenizer_datasets[\"test\"],\n",
    "    compute_metrics=eval_function,\n",
    "    processing_class=processor,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = test_image.convert(\"RGB\")\n",
    "pixel_values = processor(images=test_image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_token = model.generate(pixel_values.to(model.device))\n",
    "answer_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.batch_decode(answer_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
