{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零训练一个小的bloom模型\n",
    "\n",
    "注意事项: transformers版本有问题，\n",
    "目前使用这个是可以的\n",
    "`pip install transformers==4.45.2`\n",
    "版本有问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```requirements.txt\n",
    "absl-py==2.1.0\n",
    "accelerate==1.2.1\n",
    "aiohappyeyeballs==2.4.4\n",
    "aiohttp==3.11.11\n",
    "aiosignal==1.3.2\n",
    "anyio==4.7.0\n",
    "argon2-cffi==23.1.0\n",
    "argon2-cffi-bindings==21.2.0\n",
    "arrow==1.3.0\n",
    "asttokens==3.0.0\n",
    "async-lru==2.0.4\n",
    "attrs==24.3.0\n",
    "babel==2.16.0\n",
    "beautifulsoup4==4.12.3\n",
    "bleach==6.2.0\n",
    "certifi==2024.12.14\n",
    "cffi==1.17.1\n",
    "charset-normalizer==3.4.0\n",
    "comm==0.2.2\n",
    "datasets==3.2.0\n",
    "debugpy==1.8.11\n",
    "decorator==5.1.1\n",
    "defusedxml==0.7.1\n",
    "dill==0.3.8\n",
    "executing==2.1.0\n",
    "fastjsonschema==2.21.1\n",
    "filelock==3.16.1\n",
    "fqdn==1.5.1\n",
    "frozenlist==1.5.0\n",
    "fsspec==2024.9.0\n",
    "grpcio==1.68.1\n",
    "h11==0.14.0\n",
    "httpcore==1.0.7\n",
    "httpx==0.28.1\n",
    "huggingface-hub==0.27.0\n",
    "idna==3.10\n",
    "ipykernel==6.29.5\n",
    "ipython==8.31.0\n",
    "ipywidgets==8.1.5\n",
    "isoduration==20.11.0\n",
    "jedi==0.19.2\n",
    "Jinja2==3.1.4\n",
    "json5==0.10.0\n",
    "jsonpointer==3.0.0\n",
    "jsonschema==4.23.0\n",
    "jsonschema-specifications==2024.10.1\n",
    "jupyter==1.1.1\n",
    "jupyter-console==6.6.3\n",
    "jupyter-events==0.11.0\n",
    "jupyter-lsp==2.2.5\n",
    "jupyter_client==8.6.3\n",
    "jupyter_core==5.7.2\n",
    "jupyter_server==2.14.2\n",
    "jupyter_server_terminals==0.5.3\n",
    "jupyterlab==4.3.4\n",
    "jupyterlab_pygments==0.3.0\n",
    "jupyterlab_server==2.27.3\n",
    "jupyterlab_widgets==3.0.13\n",
    "Markdown==3.7\n",
    "MarkupSafe==3.0.2\n",
    "matplotlib-inline==0.1.7\n",
    "mistune==3.0.2\n",
    "mpmath==1.3.0\n",
    "multidict==6.1.0\n",
    "multiprocess==0.70.16\n",
    "nbclient==0.10.2\n",
    "nbconvert==7.16.4\n",
    "nbformat==5.10.4\n",
    "nest-asyncio==1.6.0\n",
    "networkx==3.4.2\n",
    "notebook==7.3.1\n",
    "notebook_shim==0.2.4\n",
    "numpy==2.2.0\n",
    "nvidia-cublas-cu12==12.4.5.8\n",
    "nvidia-cuda-cupti-cu12==12.4.127\n",
    "nvidia-cuda-nvrtc-cu12==12.4.127\n",
    "nvidia-cuda-runtime-cu12==12.4.127\n",
    "nvidia-cudnn-cu12==9.1.0.70\n",
    "nvidia-cufft-cu12==11.2.1.3\n",
    "nvidia-curand-cu12==10.3.5.147\n",
    "nvidia-cusolver-cu12==11.6.1.9\n",
    "nvidia-cusparse-cu12==12.3.1.170\n",
    "nvidia-nccl-cu12==2.21.5\n",
    "nvidia-nvjitlink-cu12==12.4.127\n",
    "nvidia-nvtx-cu12==12.4.127\n",
    "overrides==7.7.0\n",
    "packaging==24.2\n",
    "pandas==2.2.3\n",
    "pandocfilters==1.5.1\n",
    "parso==0.8.4\n",
    "pexpect==4.9.0\n",
    "platformdirs==4.3.6\n",
    "prometheus_client==0.21.1\n",
    "prompt_toolkit==3.0.48\n",
    "propcache==0.2.1\n",
    "protobuf==5.29.2\n",
    "psutil==6.1.1\n",
    "ptyprocess==0.7.0\n",
    "pure_eval==0.2.3\n",
    "pyarrow==18.1.0\n",
    "pycparser==2.22\n",
    "Pygments==2.18.0\n",
    "python-dateutil==2.9.0.post0\n",
    "python-json-logger==3.2.1\n",
    "pytz==2024.2\n",
    "PyYAML==6.0.2\n",
    "pyzmq==26.2.0\n",
    "referencing==0.35.1\n",
    "regex==2024.11.6\n",
    "requests==2.32.3\n",
    "rfc3339-validator==0.1.4\n",
    "rfc3986-validator==0.1.1\n",
    "rpds-py==0.22.3\n",
    "safetensors==0.4.5\n",
    "Send2Trash==1.8.3\n",
    "setuptools==75.6.0\n",
    "six==1.17.0\n",
    "sniffio==1.3.1\n",
    "soupsieve==2.6\n",
    "stack-data==0.6.3\n",
    "sympy==1.13.1\n",
    "tensorboard==2.18.0\n",
    "tensorboard-data-server==0.7.2\n",
    "terminado==0.18.1\n",
    "tinycss2==1.4.0\n",
    "tokenizers==0.20.3\n",
    "torch==2.5.1\n",
    "tornado==6.4.2\n",
    "tqdm==4.67.1\n",
    "traitlets==5.14.3\n",
    "transformers==4.45.2\n",
    "triton==3.1.0\n",
    "types-python-dateutil==2.9.0.20241206\n",
    "typing_extensions==4.12.2\n",
    "tzdata==2024.2\n",
    "uri-template==1.3.0\n",
    "urllib3==2.2.3\n",
    "wcwidth==0.2.13\n",
    "webcolors==24.11.1\n",
    "webencodings==0.5.1\n",
    "websocket-client==1.8.0\n",
    "Werkzeug==3.1.3\n",
    "widgetsnbextension==4.0.13\n",
    "xxhash==3.5.0\n",
    "yarl==1.18.3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BloomConfig, BloomForCausalLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForLanguageModeling, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"shaowenchen/wiki_zh\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[\"train\"].train_test_split(test_size=10, seed=42)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于bloom自带的分词器有25w 这里为了简单，用一下Bert-base-chinese的分词器  事实上会有很多问题(OOV等)，但是这样可以节省资源\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\", model_max_length=2048)  # 长度修改一下 毕竟生成模型\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\"bos_token\": \"[bos_token]\", \"eos_token\": \"[eos_token]\"}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bloom_config = BloomConfig(vocab_size=tokenizer.vocab_size+len(special_tokens_dict), hidden_size=1536, n_layer=18, n_head=12, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "bloom_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BloomForCausalLM(bloom_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(param.numel() for name, param in model.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单看下生成效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tokenizer.bos_token + \"孙中山\"\n",
    "print(input_text)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n",
    "print(inputs)\n",
    "# inputs = {k:v.to(model.device) for k, v in inputs.items() if k != \"token_type_ids\"}[]\n",
    "output = model.generate(inputs, max_new_tokens=20)\n",
    "print(output)\n",
    "print(tokenizer.decode(output[0].cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正式开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = ds[\"train\"].select(range(10))\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFuntion(example):\n",
    "    example = [tokenizer.bos_token + text + tokenizer.eos_token for text in example[\"text\"]]\n",
    "    result = tokenizer(example, truncation=True, add_special_tokens=False)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_sample_data = sample_data.map(processFuntion, batched=True, remove_columns=sample_data.column_names)\n",
    "tokenizer_sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer_sample_data[0][\"input_ids\"]\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(processFuntion, batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"output/generate_model_train\",\n",
    "    max_steps=12000,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    fp16=True,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    warmup_steps=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset=ds[\"train\"], batch_size=2, collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=False))\n",
    "for item in dataloader:\n",
    "    print(item.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 看下训练完的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"output/generate_model_train/checkpoint-4000\"\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tokenizer.bos_token + \"孙中山是一位\"\n",
    "print(input_text)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"].to(model.device)\n",
    "print(inputs)\n",
    "# inputs = {k:v.to(model.device) for k, v in inputs.items() if k != \"token_type_ids\"}[]\n",
    "output = model.generate(inputs, max_new_tokens=100)\n",
    "print(output)\n",
    "print(tokenizer.decode(output[0].cpu().numpy().tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
