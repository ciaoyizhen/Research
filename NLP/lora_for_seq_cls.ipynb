{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lora微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! 在下面运行过程中  请先执行这段代码 后面就不用在执行了 里面的变量在下面的代码中会有\n",
    "\"\"\"\n",
    "checkpoint = \"google-bert/bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "unfinetune_model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=len(id2label), label2id=label2id, id2label=id2label)  # 这个数据集是7个,主要是针对语言的\n",
    "\n",
    "checkpoint = \"output/save_model\"\n",
    "unfinetune_model.save_pretrained(checkpoint)  # 这里是提前保存  防止因为参数是随机生成的  不好比较\n",
    "tokenizer.save_pretrained(checkpoint)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"agentlans/chinese-classification\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个数据有点多，我们就拿他的验证集做训练集 测试集当验证集\n",
    "ds[\"train\"] = ds[\"validation\"]\n",
    "ds.pop(\"validation\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拿到类别\n",
    "labels = set(ds[\"train\"][\"label\"])\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = ds[\"train\"].select(range(10))\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #! 提前保存模型\n",
    "# checkpoint = \"google-bert/bert-base-chinese\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# unfinetune_model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=len(id2label), label2id=label2id, id2label=id2label)  # 这个数据集是7个,主要是针对语言的\n",
    "\n",
    "checkpoint = \"output/save_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "unfinetune_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "def processFunciton(examples):\n",
    "    result = tokenizer(examples[\"text\"], truncation=True)\n",
    "    labels = examples[\"label\"]\n",
    "    labels = list(map(lambda x: label2id[x], labels))\n",
    "    result[\"labels\"] = labels\n",
    "    return result\n",
    "\n",
    "tokenizerd_sample_data = sample_data.map(processFunciton, batched=True)\n",
    "tokenizerd_sample_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确认一下 label是否转换正确\n",
    "for item in tokenizerd_sample_data:\n",
    "    label = item[\"label\"]\n",
    "    label_id = item[\"labels\"]\n",
    "    assert id2label[label_id] == label, f\"错误:{label} != {id2label[label_id]} \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 看一下还没训练的结果(实际训练中 可以忽略)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "checkpoint = \"output/save_model\"\n",
    "unfinetune_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)  # 这个数据集是7个,主要是针对语言的\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "pipe = pipeline(\"text-classification\", model=unfinetune_model, tokenizer=tokenizer)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"都唔明點解成日都會有人拎呢樣嚟打飛機\"\n",
    "label = \"yue\"\n",
    "result = pipe(text)\n",
    "print(result)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始lora训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(processFunciton, batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"output/save_model\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(task_type=TaskType.SEQ_CLS)  # 这个地方是想把module_to_save设为空  但是事实上不行\n",
    "lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"output/lora_seq_cls\",\n",
    "    per_device_train_batch_size=24,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_steps=3000,\n",
    "    save_steps=100,\n",
    "    save_safetensors=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    data_seed=42,\n",
    "    logging_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推理部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"output/save_model\"\n",
    "lora_checkpoint = \"output/lora_seq_cls/checkpoint-300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfinetune_model = AutoModelForSequenceClassification.from_pretrained(checkpoint).to(\"cuda:0\")  # 这个数据集是7个,主要是针对语言的\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    text = \"都唔明點解成日都會有人拎呢樣嚟打飛機\"\n",
    "    label = \"yue\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = {k:v.to(unfinetune_model.device)for k, v in inputs.items()}\n",
    "    result = unfinetune_model(**inputs)\n",
    "    logits = result.logits\n",
    "    result = torch.nn.functional.softmax(logits, -1).max().item()\n",
    "    print(f\"score: {result}\")\n",
    "    result = logits.argmax(-1).cpu()\n",
    "    result = result.cpu().item()\n",
    "    print(unfinetune_model.config.id2label[result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unfinetune_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里其实可以不用接收这个返回 也能直接用于lora\n",
    "unfinetune_model = PeftModelForSequenceClassification.from_pretrained(unfinetune_model, lora_checkpoint)\n",
    "unfinetune_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfinetune_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    text = \"都唔明點解成日都會有人拎呢樣嚟打飛機\"\n",
    "    label = \"yue\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = {k:v.to(unfinetune_model.device)for k, v in inputs.items()}\n",
    "    result = unfinetune_model(**inputs)\n",
    "    logits = result.logits\n",
    "    result = torch.nn.functional.softmax(logits, -1).max().item()\n",
    "    print(f\"score: {result}\")\n",
    "    result = logits.argmax(-1).cpu()\n",
    "    result = result.cpu().item()\n",
    "    print(unfinetune_model.config.id2label[result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with unfinetune_model.disable_adapter():\n",
    "    with torch.inference_mode():\n",
    "        text = \"都唔明點解成日都會有人拎呢樣嚟打飛機\"\n",
    "        label = \"yue\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        inputs = {k:v.to(unfinetune_model.device)for k, v in inputs.items()}\n",
    "        result = unfinetune_model(**inputs)\n",
    "        logits = result.logits\n",
    "        result = torch.nn.functional.softmax(logits, -1).max().item()\n",
    "        print(f\"score: {result}\")\n",
    "        result = logits.argmax(-1).cpu()\n",
    "        result = result.cpu().item()\n",
    "        print(unfinetune_model.config.id2label[result])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
