{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这里演示三种tokenizer的获取的方法\n",
    "\n",
    "+ 现有词表构建tokenizer\n",
    "+ BPE训练tokenizer\n",
    "+ WordPiece训练tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于现有词表进行构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词表链接 https://github.com/bedlate/cn-corpus/blob/master/现代汉语常用字表.xls\n",
    "data = pd.read_excel(r\"C:/Users/Administrator/Downloads/现代汉语常用字表.xls\", skiprows=4)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"[PAD]\"}\n",
    "label2id = {\"[PAD]\": 0}\n",
    "for _, row in data.iterrows():\n",
    "    id_ = row[\"ID\"]\n",
    "    label = row[\"汉字\"]\n",
    "    id2label[id_] = label\n",
    "    label2id[label] = id_\n",
    "else:\n",
    "    index = id_ + 1\n",
    "    id2label[index] = \"[UNK]\"\n",
    "    label2id[\"[UNK]\"] = index\n",
    "    index += 1\n",
    "    id2label[index] = \"[BOS]\"\n",
    "    label2id[\"[BOS]\"] = index\n",
    "    index += 1\n",
    "    id2label[index] = \"[EOS]\"\n",
    "    label2id[\"[EOS]\"] = index\n",
    "\n",
    "    \n",
    "    # 手动添加一些额外的符号\n",
    "    #! 这里请特别注意省略号是两个符号，要处理成一个符号\n",
    "    sign = \"。，？《》；“”：、…！123456789,.?/;'\\\"!~()（）*&^￥@|\\#ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"  # 自己补充吧。。。    \n",
    "    for item in sign:\n",
    "        index += 1\n",
    "        id2label[index] = item\n",
    "        label2id[item] = index\n",
    "    \n",
    "    \n",
    "print(id2label)\n",
    "print(len(id2label))\n",
    "print(label2id)\n",
    "print(len(label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "from tokenizers.decoders import Strip\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = BertNormalizer(clean_text=True, handle_chinese_chars=True)\n",
    "model = WordLevel(label2id, unk_token=\"[UNK]\")\n",
    "tokenizer = Tokenizer(model)\n",
    "tokenizer.normalizer = normalizer\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.decoder = Strip()\n",
    "tokenizer.enable_padding()\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "print(tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(id2label))\n",
    "tokenizer.decode([len(id2label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\n",
    "    \"eos_token\": \"[EOS]\",\n",
    "    \"bos_token\": \"[BOS]\",\n",
    "    \"pad_token\": \"[PAD]\"\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens_dict=special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"[BOS]你好。我是A昐[EOS][PAD]\")[\"input_ids\"]\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练数据来自https://github.com/ciaoyizhen/crawler_for_generate_model\n",
    "data_file_list = [\n",
    "    r\"C:/Users/Administrator/Downloads/斗破苍穹.txt\",\n",
    "    r\"C:/Users/Administrator/Downloads/武动乾坤.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for data_file in data_file_list:\n",
    "    with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(line)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import ByteLevel as PreByteLevel\n",
    "from tokenizers.decoders import ByteLevel as PostByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.models import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = PreByteLevel(add_prefix_space=False)  # 预处理时，将词转化为编码,不然未见过的词无法处理\n",
    "tokenizer.decoder = PostByteLevel()  # 不加会导致decode回来的时候 仍然是乱码的\n",
    "special_tokens = [\n",
    "    \"<assistant>\",\n",
    "    \"<user>\",\n",
    "    \"<system>\",\n",
    "    \"<eos_token>\",\n",
    "]\n",
    "trainer = BpeTrainer(vocab_size=25000, min_frequency=2, special_tokens=special_tokens, show_progress=True)\n",
    "tokenizer.train_from_iterator(data, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.encode(\"萧玄跟古元同辈，萧炎是萧玄的后代，熏儿是古元的女儿，萧炎跟熏儿谈恋爱？\").ids\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"萧玄跟古元同辈，萧炎是萧玄的后代，熏儿是古元的女儿，萧炎跟熏儿谈恋爱？\")[\"input_ids\"]\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"囍\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPiece训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练数据来自https://github.com/ciaoyizhen/crawler_for_generate_model\n",
    "data_file_list = [\n",
    "    r\"C:/Users/Administrator/Downloads/斗破苍穹.txt\",\n",
    "    r\"C:/Users/Administrator/Downloads/武动乾坤.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for data_file in data_file_list:\n",
    "    with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(line)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.decoders import WordPiece as DecoderWordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(model=WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "tokenizer.normalizer = BertNormalizer(handle_chinese_chars=False)  # pre_tokenizer在根据空格切，这个参数会在中文旁边生成空格\n",
    "tokenizer.decoder = DecoderWordPiece()\n",
    "trainer = WordPieceTrainer(vocab_size=25000, show_progress=True, special_tokens=[\"[UNK]\", \"[BOS]\", \"[EOS]\"])\n",
    "tokenizer.train_from_iterator(data, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"[UNK][BOS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.encode(\"萧玄跟古元同辈，萧炎是萧玄的后代，熏儿是古元的女儿，萧炎跟熏儿谈恋爱？\").ids\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"[BOS]\",\n",
    "    \"eos_token\": \"[EOS]\"\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens_dict=special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"囍\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"萧炎\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 题外话内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")\n",
    "text = \"如何解决编码回来自带的空格\"\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.decoders import Strip\n",
    "\n",
    "tokenizer.backend_tokenizer.decoder = Strip()\n",
    "text = \"如何解决编码回来自带的空格\"\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
