{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"qgyd2021/chinese_ner_sft\", \"CMeEE\", trust_remote_code=True) \n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[\"train\"].train_test_split(test_size=10)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = ds[\"train\"].select(range(10))\n",
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sample_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = a[\"text\"]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = a[\"entities\"]\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = entities[\"start_idx\"]\n",
    "end_idx = entities[\"end_idx\"]\n",
    "entity_text = entities[\"entity_text\"]\n",
    "text = a[\"text\"]\n",
    "\n",
    "for start, end, true_text in zip(start_idx, end_idx, entity_text):\n",
    "    label_text = text[start:end+1]\n",
    "    assert label_text == true_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(examples):\n",
    "    label2name = {}\n",
    "    for item in examples:\n",
    "        entities = item[\"entities\"]\n",
    "        entity_label = entities[\"entity_label\"]\n",
    "        entity_name = entities[\"entity_names\"]\n",
    "        \n",
    "        for label, name in zip(entity_label, entity_name):\n",
    "            if label not in label2name:\n",
    "                label2name[label] = name\n",
    "            else:\n",
    "                assert label2name[label] == name, f\"{label2name[label]} != {name}\"\n",
    "                \n",
    "                \n",
    "    id2label = {}\n",
    "    label2id = {}\n",
    "\n",
    "    index = 0\n",
    "    for label_org in label2name:\n",
    "        label = \"B-\" + label_org\n",
    "        id2label[index] = label\n",
    "        label2id[label] = index\n",
    "        index += 1\n",
    "        \n",
    "        label = \"I-\" + label_org\n",
    "        id2label[index] = label\n",
    "        label2id[label] = index\n",
    "        index += 1\n",
    "    # 非实体    \n",
    "    id2label[index] = \"O\"\n",
    "    label2id[\"O\"] = index\n",
    "    \n",
    "    return label2name, label2id, id2label\n",
    "\n",
    "label2name, label2id, id2label = getLabel(ds[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelAndTokenizer(checkpoint):\n",
    "    model = AutoModelForTokenClassification.from_pretrained(checkpoint, label2id=label2id, id2label=id2label)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    return model, tokenizer\n",
    "\n",
    "checkpoint = \"google-bert/bert-base-chinese\"\n",
    "model, tokenizer = getModelAndTokenizer(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "for item in sample_dataset.select(range(2)):\n",
    "    text = item[\"text\"]\n",
    "    text_list.append(text)\n",
    "inputs = tokenizer(text_list, return_offsets_mapping=True)\n",
    "inputs[\"offset_mapping\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(examples):\n",
    "    text_list = examples[\"text\"]\n",
    "    example_label_list = []\n",
    "    entities = examples[\"entities\"]\n",
    "    for item in entities:\n",
    "        start_idx = item[\"start_idx\"]\n",
    "        end_idx = item[\"end_idx\"]\n",
    "        entity_label = item[\"entity_label\"]\n",
    "        \n",
    "        example_label_list.append((start_idx, end_idx, entity_label))\n",
    "    \n",
    "    assert len(example_label_list) == len(text_list)\n",
    "    \n",
    "    inputs = tokenizer(text_list, return_offsets_mapping=True, truncation=True, max_length=384)\n",
    "    offset_mapping_list = inputs[\"offset_mapping\"]\n",
    "    \n",
    "    total_label_list = []\n",
    "        \n",
    "    for offset_mapping, label in zip(offset_mapping_list, example_label_list):\n",
    "        \n",
    "        start_idx, end_idx, entity_label = label\n",
    "        # 先针对位置做一个映射关系\n",
    "        map_dict = {}\n",
    "        for start, end, label in zip(start_idx, end_idx, entity_label):\n",
    "            map_dict[start] = \"B-\" + label\n",
    "            for i in range(start+1, end+1):\n",
    "                map_dict[i] = \"I-\" + label\n",
    "                \n",
    "        label_list = []\n",
    "        for offset in offset_mapping:\n",
    "            start, end = offset\n",
    "            if start == end:\n",
    "                label_list.append(-100)\n",
    "            else:\n",
    "                if start not in map_dict:\n",
    "                    label_list.append(label2id[\"O\"])\n",
    "                else:\n",
    "                    label_list.append(label2id[map_dict[start]])\n",
    "        assert len(label_list) == len(offset_mapping)\n",
    "        total_label_list.append(label_list)\n",
    "        \n",
    "    inputs[\"labels\"] = total_label_list\n",
    "    return inputs\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dataset = ds.map(process_function, batched=True)\n",
    "tokenizer_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"output/CMeEE\",\n",
    "    logging_steps=20,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenizer_dataset[\"train\"],\n",
    "    eval_dataset=tokenizer_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", ignore_labels=[\"O\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = ds[\"test\"]\n",
    "text = test_data[0][\"text\"]\n",
    "entities = test_data[0][\"entities\"]\n",
    "text, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
