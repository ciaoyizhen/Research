{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现简易版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForQuestionAnswering, AutoTokenizer, DefaultDataCollator, DataCollatorWithPadding\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"hfl/cmrc2018\")\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_datasets = datasets[\"train\"].select(range(10))\n",
    "print(sample_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"hfl/chinese-macbert-base\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint, num_labels=2)  # 问答只有开始和结束\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(examples):\n",
    "    answer_list = examples[\"answers\"]\n",
    "    \n",
    "    tokenized_examples = tokenizer(\n",
    "        text=examples[\"question\"],\n",
    "        text_pair=examples[\"context\"],\n",
    "        truncation=\"only_second\",  # 如果问题已经超过最大长度上线，是会报错的,\n",
    "        max_length=384,\n",
    "        padding=\"longest\",\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "        \n",
    "    offset_list = tokenized_examples[\"offset_mapping\"]\n",
    "    start_positions_list = []\n",
    "    end_positions_list = []\n",
    "    for answer, offset in zip(answer_list, offset_list):\n",
    "        # 定位答案在字符串中的位置\n",
    "        answer_text = answer[\"text\"][0]\n",
    "        answer_start = answer[\"answer_start\"][0]\n",
    "        answer_end = answer_start + len(answer_text)\n",
    "        \n",
    "        # 定位context在token的范围\n",
    "        start_context_index = tokenized_examples.sequence_ids().index(1)\n",
    "        end_context_index = tokenized_examples.sequence_ids().index(None, start_context_index)\n",
    "        \n",
    "        token_start = None\n",
    "        token_end = None    \n",
    "        # 找到答案对应的token位置 只在给定的上下文范围内找\n",
    "        for i, (offset_start, offset_end) in enumerate(offset[start_context_index:end_context_index]):\n",
    "            if offset_start == answer_start:\n",
    "                token_start = i + start_context_index\n",
    "            elif offset_end == answer_end:\n",
    "                token_end = i + start_context_index\n",
    "        \n",
    "        # 如果没找到答案，则使用cls\n",
    "        if token_end is None or token_start is None:\n",
    "            token_start = 0\n",
    "            token_end = 0\n",
    "        start_positions_list.append(token_start)\n",
    "        end_positions_list.append(token_end)\n",
    "    tokenized_examples[\"start_positions\"] = start_positions_list\n",
    "    tokenized_examples[\"end_positions\"] = end_positions_list\n",
    "    return tokenized_examples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(process, batched=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenizer_datasets = tokenized_datasets[\"train\"]\n",
    "start_token = test_tokenizer_datasets[0][\"start_positions\"]\n",
    "end_token = test_tokenizer_datasets[0][\"end_positions\"]\n",
    "\n",
    "answer = test_tokenizer_datasets[0][\"input_ids\"][start_token:end_token+1]\n",
    "tokenizer.decode(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args  = TrainingArguments(\n",
    "    \"output\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    # data_collator=DefaultDataCollator(),\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "checkpoint = \"output\\checkpoint-951\"\n",
    "pipe = pipeline(\"question-answering\",  model=checkpoint , device=\"cuda:0\")\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"小明在那里上学\"\n",
    "context = \"小明在北京上学\"\n",
    "print(pipe(question=question, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"周杰伦什么时候出生的\"\n",
    "context = \"周杰伦（1979年1月18日—），台湾创作男歌手、演员、词曲作家及制作人。其音乐风行于大中华地区及全球各地的华人社群，并对华语乐坛产生重大影响，也是史上最具影响力及最著名的华语歌手之一[3][4][5]。\"\n",
    "print(pipe(question=question, context=context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# overlap版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, AutoTokenizer, DataCollatorWithPadding, Trainer\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"hfl/cmrc2018\")\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"hfl/chinese-macbert-base\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint, num_labels=2)  # 问答只有开始和结束\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_datasets = datasets[\"train\"].select(range(10))\n",
    "sample_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer_list = sample_datasets[\"answers\"]\n",
    "context_list = sample_datasets[\"context\"]\n",
    "id_list = sample_datasets[\"id\"]  # 如果数据没有 得使用with_transformers给他生成id\n",
    "tokenized_sample_datasets = tokenizer(\n",
    "    text=sample_datasets[\"question\"],\n",
    "    text_pair=sample_datasets[\"context\"],\n",
    "    truncation=\"only_second\",\n",
    "    padding=\"longest\",\n",
    "    return_offsets_mapping=True,\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=128,\n",
    "    max_length=384\n",
    ")\n",
    "\n",
    "\n",
    "# 确定每条的对应关系  主要是为了答案能对的上\n",
    "overflow_to_sample_mapping = tokenized_sample_datasets[\"overflow_to_sample_mapping\"]\n",
    "input_ids = tokenized_sample_datasets[\"input_ids\"]\n",
    "offset_mapping = tokenized_sample_datasets[\"offset_mapping\"]  # overflow之后  offset_mapping是保持原来的句子顺序的\n",
    "\n",
    "start_position_list = []\n",
    "end_position_list = []\n",
    "for i, (index, offset, input_id) in enumerate(zip(overflow_to_sample_mapping, offset_mapping, input_ids)):\n",
    "    # 确定上下文的token位置\n",
    "    start_context_index = tokenized_sample_datasets.sequence_ids(i).index(1)\n",
    "    end_context_index = tokenized_sample_datasets.sequence_ids(i).index(None, start_context_index)\n",
    "    \n",
    "    # 确定答案的字符长度\n",
    "    answer = answer_list[index]\n",
    "    answer_text = answer[\"text\"][0]\n",
    "    answer_start = answer[\"answer_start\"][0]\n",
    "    answer_end = answer_start + len(answer_text)\n",
    "    \n",
    "    # 确定答案的token位置\n",
    "    start_token_index = None\n",
    "    end_token_index = None\n",
    "    for i, (start_token, end_token) in enumerate(offset[start_context_index:end_context_index]):\n",
    "        if start_token == answer_start:\n",
    "            start_token_index = i + start_context_index\n",
    "        elif end_token == answer_end:\n",
    "            end_token_index = i + start_context_index\n",
    "    \n",
    "    if start_token_index is None or end_token_index is None:\n",
    "        start_token_index = 0\n",
    "        end_token_index = 0\n",
    "    \n",
    "    start_position_list.append(start_token_index)\n",
    "    end_position_list.append(end_token_index)\n",
    "print(len(tokenized_sample_datasets[\"input_ids\"]))\n",
    "print(len(start_position_list))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(examples):\n",
    "    answer_list = examples[\"answers\"]\n",
    "    \n",
    "    tokenized_examples = tokenizer(\n",
    "        text=examples[\"question\"],\n",
    "        text_pair=examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        padding=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_overflowing_tokens=True,  # 用这个参数 必须同一个长度 padding 必须长度相同\n",
    "        stride=128,\n",
    "        max_length=384\n",
    "    )\n",
    "\n",
    "\n",
    "    # 确定每条的对应关系  主要是为了答案能对的上\n",
    "    overflow_to_sample_mapping = tokenized_examples[\"overflow_to_sample_mapping\"]\n",
    "    offset_mapping = tokenized_examples[\"offset_mapping\"]  # overflow之后  offset_mapping是保持原来的句子顺序的\n",
    "\n",
    "    start_position_list = []\n",
    "    end_position_list = []\n",
    "    for i, (index, offset, ) in enumerate(zip(overflow_to_sample_mapping, offset_mapping)):\n",
    "        # 确定上下文的token位置\n",
    "        start_context_index = tokenized_examples.sequence_ids(i).index(1)\n",
    "        end_context_index = tokenized_examples.sequence_ids(i).index(None, start_context_index)\n",
    "        \n",
    "        # 确定答案的字符长度\n",
    "        answer = answer_list[index]\n",
    "        answer_text = answer[\"text\"][0]\n",
    "        answer_start = answer[\"answer_start\"][0]\n",
    "        answer_end = answer_start + len(answer_text)\n",
    "        \n",
    "        # 确定答案的token位置\n",
    "        start_token_index = None\n",
    "        end_token_index = None\n",
    "        for i, (start_token, end_token) in enumerate(offset[start_context_index:end_context_index]):\n",
    "            if start_token == answer_start:\n",
    "                start_token_index = i + start_context_index\n",
    "            elif end_token == answer_end:\n",
    "                end_token_index = i + start_context_index\n",
    "        \n",
    "        if start_token_index is None or end_token_index is None:\n",
    "            start_token_index = 0\n",
    "            end_token_index = 0\n",
    "        \n",
    "        start_position_list.append(start_token_index)\n",
    "        end_position_list.append(end_token_index)\n",
    "    tokenized_examples[\"start_positions\"] = start_position_list\n",
    "    tokenized_examples[\"end_positions\"] = end_position_list\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(process, batched=True, remove_columns=datasets[\"train\"].column_names)  # 这里一定要移除列名，因为overflow会生成数据，导致行数不匹配\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args  = TrainingArguments(\n",
    "    \"output\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    # data_collator=DefaultDataCollator(),\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "checkpoint = \"output/checkpoint-450\"\n",
    "pipe = pipeline(\"question-answering\",  model=checkpoint , device=\"cuda:0\")\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"小明在那里上学\"\n",
    "context = \"小明在北京上学\"\n",
    "print(pipe(question=question, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"周杰伦什么时候出生的\"\n",
    "context = \"周杰伦（1979年1月18日—），台湾创作男歌手、演员、词曲作家及制作人。其音乐风行于大中华地区及全球各地的华人社群，并对华语乐坛产生重大影响，也是史上最具影响力及最著名的华语歌手之一[3][4][5]。\"\n",
    "print(pipe(question=question, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"为什么周杰伦在流行音乐创作方面如鱼得水\"\n",
    "context = \"\"\"\n",
    "周杰伦在台湾台北县林口乡[注 1]出生长大[12]，为家中的独生子[13][14]。父亲周耀中任教于芦洲国中，教授生物[15]；母亲叶惠美则是林口国中美术老师。14岁时父母离异，由父亲担任监护人，年满18岁后选择与母亲共同生活[16]。周杰伦曾在台湾民视新闻台由胡婉玲主持的节目《台湾演义》专访中澄清《爸，我回来了》这首歌只是对社会上家暴现象的感慨，并非指涉父母间的状况；父亲的亲戚也曾质疑过他，因此还为特别向亲戚们澄清和道歉过[17]。\n",
    "\n",
    "周杰伦自小对音乐表现出浓厚的兴趣，并且喜欢模仿歌星、演员表演和变魔术。3岁开始学习钢琴。周杰伦国小时住在台北市光华商场附近，就读忠孝国小。国中时就读金华国中[18]，此时期他的父母因长年争执而决议离婚，使周杰伦的性情大受影响。除了音乐外，周杰伦热爱篮球，在国中曾参加过篮球队。\n",
    "\n",
    "高中就读于台北县私立淡江中学第一届音乐科（本来是想报考华冈艺校，但错过了报名时间，幸好淡江中学恰巧新设了音乐科），主修钢琴，为将来的音乐发展打下了深厚的基础[19]。这时的他因正值青春期，常常秀琴技想吸引女同学的注意。但学科成绩不甚理想，故高中毕业时，大学联考落榜。又因患有僵直性脊椎炎，依据台湾兵役制度得以免服义务兵役[20]。\n",
    "\n",
    "周杰伦曾表示少年时受到香港乐坛“四大天王”之一张学友的专辑《吻别》的影响，从而喜欢并开始专注于流行音乐[21]。另外他也透露过，除了张学友以外，肖邦、李恕权与史帝夫·汪达也是他童年及成长时影响他很深的人：05年的专辑更以《十一月的肖邦》为标题，07年电影《不能说的秘密》的斗琴多处桥段和肖邦有关，示意对他致敬；李恕权每回出现在电视上，周杰伦便会在电视机面前模仿他；而史帝夫·汪达有一首《I Just Called to Say I love You》是他的婶婶曾在他叔父的葬礼中播放的歌曲。由于他的音乐基础扎实，令其在流行音乐创作方面如鱼得水。\"\"\"\n",
    "print(pipe(question=question, context=context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
