{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现简易版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForQuestionAnswering, AutoTokenizer, DefaultDataCollator, DataCollatorWithPadding\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since hfl/cmrc2018 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/ubuntu/.cache/huggingface/datasets/hfl___cmrc2018/default/0.0.0/137f2c45a24275fb68f6961c4d357f46288886aa (last modified on Fri Dec 13 13:57:10 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'context', 'question', 'answers'],\n",
      "        num_rows: 10142\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'context', 'question', 'answers'],\n",
      "        num_rows: 3219\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'context', 'question', 'answers'],\n",
      "        num_rows: 1002\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\"hfl/cmrc2018\")\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'context', 'question', 'answers'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "sample_datasets = datasets[\"train\"].select(range(10))\n",
    "print(sample_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"hfl/chinese-macbert-base\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint, num_labels=2)  # 问答只有开始和结束\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(examples):\n",
    "    answer_list = examples[\"answers\"]\n",
    "    \n",
    "    tokenized_examples = tokenizer(\n",
    "        text=examples[\"question\"],\n",
    "        text_pair=examples[\"context\"],\n",
    "        truncation=\"only_second\",  # 如果问题已经超过最大长度上线，是会报错的,\n",
    "        max_length=384,\n",
    "        padding=\"longest\",\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "        \n",
    "    offset_list = tokenized_examples[\"offset_mapping\"]\n",
    "    start_positions_list = []\n",
    "    end_positions_list = []\n",
    "    for answer, offset in zip(answer_list, offset_list):\n",
    "        # 定位答案在字符串中的位置\n",
    "        answer_text = answer[\"text\"][0]\n",
    "        answer_start = answer[\"answer_start\"][0]\n",
    "        answer_end = answer_start + len(answer_text)\n",
    "        \n",
    "        # 定位context在token的范围\n",
    "        start_context_index = tokenized_examples.sequence_ids().index(1)\n",
    "        end_context_index = tokenized_examples.sequence_ids().index(None, start_context_index)\n",
    "        \n",
    "        token_start = None\n",
    "        token_end = None    \n",
    "        # 找到答案对应的token位置 只在给定的上下文范围内找\n",
    "        for i, (offset_start, offset_end) in enumerate(offset[start_context_index:end_context_index]):\n",
    "            if offset_start == answer_start:\n",
    "                token_start = i + start_context_index\n",
    "            elif offset_end == answer_end:\n",
    "                token_end = i + start_context_index\n",
    "        \n",
    "        # 如果没找到答案，则使用cls\n",
    "        if token_end is None or token_start is None:\n",
    "            token_start = 0\n",
    "            token_end = 0\n",
    "        start_positions_list.append(token_start)\n",
    "        end_positions_list.append(token_end)\n",
    "    tokenized_examples[\"start_positions\"] = start_positions_list\n",
    "    tokenized_examples[\"end_positions\"] = end_positions_list\n",
    "    return tokenized_examples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93854ac0326f4a0eb40598c1e5781624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bba0d1f51143178dbe0f976ae732bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7779527f1aa4adc9bb1c6b5f883878f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'context', 'question', 'answers', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions'],\n",
      "        num_rows: 10142\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'context', 'question', 'answers', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions'],\n",
      "        num_rows: 3219\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'context', 'question', 'answers', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions'],\n",
      "        num_rows: 1002\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(process, batched=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1963 年'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenizer_datasets = tokenized_datasets[\"train\"]\n",
    "start_token = test_tokenizer_datasets[0][\"start_positions\"]\n",
    "end_token = test_tokenizer_datasets[0][\"end_positions\"]\n",
    "\n",
    "answer = test_tokenizer_datasets[0][\"input_ids\"][start_token:end_token+1]\n",
    "tokenizer.decode(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args  = TrainingArguments(\n",
    "    \"output\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    # data_collator=DefaultDataCollator(),\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/code/jupyter_code/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 09:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.287100</td>\n",
       "      <td>1.748305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.748700</td>\n",
       "      <td>1.357984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.422500</td>\n",
       "      <td>1.348430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.302800</td>\n",
       "      <td>1.286842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.297200</td>\n",
       "      <td>1.300590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.065700</td>\n",
       "      <td>1.359534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.041600</td>\n",
       "      <td>1.299335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.093400</td>\n",
       "      <td>1.250832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=240, training_loss=1.6052495082219442, metrics={'train_runtime': 589.6174, 'train_samples_per_second': 51.603, 'train_steps_per_second': 0.407, 'total_flos': 5962661340337152.0, 'train_loss': 1.6052495082219442, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.question_answering.QuestionAnsweringPipeline object at 0x7f781dbd61e0>\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "checkpoint = \"output/checkpoint-240\"\n",
    "pipe = pipeline(\"question-answering\",  model=checkpoint , device=\"cuda:0\")\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.49330437183380127, 'start': 3, 'end': 5, 'answer': '北京'}\n"
     ]
    }
   ],
   "source": [
    "question = \"小明在那里上学\"\n",
    "context = \"小明在北京上学\"\n",
    "print(pipe(question=question, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.016886375844478607, 'start': 4, 'end': 14, 'answer': '1979年1月18日'}\n"
     ]
    }
   ],
   "source": [
    "question = \"周杰伦什么时候出生的\"\n",
    "context = \"周杰伦（1979年1月18日—），台湾创作男歌手、演员、词曲作家及制作人。其音乐风行于大中华地区及全球各地的华人社群，并对华语乐坛产生重大影响，也是史上最具影响力及最著名的华语歌手之一[3][4][5]。\"\n",
    "print(pipe(question=question, context=context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# overlap版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, AutoTokenizer, DataCollatorWithPadding, Trainer\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since hfl/cmrc2018 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/ubuntu/.cache/huggingface/datasets/hfl___cmrc2018/default/0.0.0/137f2c45a24275fb68f6961c4d357f46288886aa (last modified on Fri Dec 13 16:46:44 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'context', 'question', 'answers'],\n",
      "        num_rows: 10142\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'context', 'question', 'answers'],\n",
      "        num_rows: 3219\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'context', 'question', 'answers'],\n",
      "        num_rows: 1002\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\"hfl/cmrc2018\")\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"hfl/chinese-macbert-base\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint, num_labels=2)  # 问答只有开始和结束\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'context', 'question', 'answers'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_datasets = datasets[\"train\"].select(range(10))\n",
    "sample_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "\n",
    "answer_list = sample_datasets[\"answers\"]\n",
    "context_list = sample_datasets[\"context\"]\n",
    "id_list = sample_datasets[\"id\"]  # 如果数据没有 得使用with_transformers给他生成id\n",
    "tokenized_sample_datasets = tokenizer(\n",
    "    text=sample_datasets[\"question\"],\n",
    "    text_pair=sample_datasets[\"context\"],\n",
    "    truncation=\"only_second\",\n",
    "    padding=\"longest\",\n",
    "    return_offsets_mapping=True,\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=128,\n",
    "    max_length=384\n",
    ")\n",
    "\n",
    "\n",
    "# 确定每条的对应关系  主要是为了答案能对的上\n",
    "overflow_to_sample_mapping = tokenized_sample_datasets[\"overflow_to_sample_mapping\"]\n",
    "input_ids = tokenized_sample_datasets[\"input_ids\"]\n",
    "offset_mapping = tokenized_sample_datasets[\"offset_mapping\"]  # overflow之后  offset_mapping是保持原来的句子顺序的\n",
    "\n",
    "start_position_list = []\n",
    "end_position_list = []\n",
    "for i, (index, offset, input_id) in enumerate(zip(overflow_to_sample_mapping, offset_mapping, input_ids)):\n",
    "    # 确定上下文的token位置\n",
    "    start_context_index = tokenized_sample_datasets.sequence_ids(i).index(1)\n",
    "    end_context_index = tokenized_sample_datasets.sequence_ids(i).index(None, start_context_index)\n",
    "    \n",
    "    # 确定答案的字符长度\n",
    "    answer = answer_list[index]\n",
    "    answer_text = answer[\"text\"][0]\n",
    "    answer_start = answer[\"answer_start\"][0]\n",
    "    answer_end = answer_start + len(answer_text)\n",
    "    \n",
    "    # 确定答案的token位置\n",
    "    start_token_index = None\n",
    "    end_token_index = None\n",
    "    for i, (start_token, end_token) in enumerate(offset[start_context_index:end_context_index]):\n",
    "        if start_token == answer_start:\n",
    "            start_token_index = i + start_context_index\n",
    "        elif end_token == answer_end:\n",
    "            end_token_index = i + start_context_index\n",
    "    \n",
    "    if start_token_index is None or end_token_index is None:\n",
    "        start_token_index = 0\n",
    "        end_token_index = 0\n",
    "    \n",
    "    start_position_list.append(start_token_index)\n",
    "    end_position_list.append(end_token_index)\n",
    "print(len(tokenized_sample_datasets[\"input_ids\"]))\n",
    "print(len(start_position_list))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(examples):\n",
    "    answer_list = examples[\"answers\"]\n",
    "    \n",
    "    tokenized_examples = tokenizer(\n",
    "        text=examples[\"question\"],\n",
    "        text_pair=examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        padding=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_overflowing_tokens=True,  # 用这个参数 必须同一个长度 padding 必须长度相同\n",
    "        stride=128,\n",
    "        max_length=384\n",
    "    )\n",
    "\n",
    "\n",
    "    # 确定每条的对应关系  主要是为了答案能对的上\n",
    "    overflow_to_sample_mapping = tokenized_examples[\"overflow_to_sample_mapping\"]\n",
    "    offset_mapping = tokenized_examples[\"offset_mapping\"]  # overflow之后  offset_mapping是保持原来的句子顺序的\n",
    "\n",
    "    start_position_list = []\n",
    "    end_position_list = []\n",
    "    for i, (index, offset, ) in enumerate(zip(overflow_to_sample_mapping, offset_mapping)):\n",
    "        # 确定上下文的token位置\n",
    "        start_context_index = tokenized_examples.sequence_ids(i).index(1)\n",
    "        end_context_index = tokenized_examples.sequence_ids(i).index(None, start_context_index)\n",
    "        \n",
    "        # 确定答案的字符长度\n",
    "        answer = answer_list[index]\n",
    "        answer_text = answer[\"text\"][0]\n",
    "        answer_start = answer[\"answer_start\"][0]\n",
    "        answer_end = answer_start + len(answer_text)\n",
    "        \n",
    "        # 确定答案的token位置\n",
    "        start_token_index = None\n",
    "        end_token_index = None\n",
    "        for i, (start_token, end_token) in enumerate(offset[start_context_index:end_context_index]):\n",
    "            if start_token == answer_start:\n",
    "                start_token_index = i + start_context_index\n",
    "            elif end_token == answer_end:\n",
    "                end_token_index = i + start_context_index\n",
    "        \n",
    "        if start_token_index is None or end_token_index is None:\n",
    "            start_token_index = 0\n",
    "            end_token_index = 0\n",
    "        \n",
    "        start_position_list.append(start_token_index)\n",
    "        end_position_list.append(end_token_index)\n",
    "    tokenized_examples[\"start_positions\"] = start_position_list\n",
    "    tokenized_examples[\"end_positions\"] = end_position_list\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee68381742145beb72a0ed14eb8f745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e3abc780c34ab28871292054913f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d331205b617c4ff0bae1c98bc3c0985b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'start_positions', 'end_positions'],\n",
      "        num_rows: 19189\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'start_positions', 'end_positions'],\n",
      "        num_rows: 6327\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'start_positions', 'end_positions'],\n",
      "        num_rows: 1988\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(process, batched=True, remove_columns=datasets[\"train\"].column_names)  # 这里一定要移除列名，因为overflow会生成数据，导致行数不匹配\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args  = TrainingArguments(\n",
    "    \"output\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    # data_collator=DefaultDataCollator(),\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/code/jupyter_code/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 38:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.497600</td>\n",
       "      <td>3.046438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.608000</td>\n",
       "      <td>1.970672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.975200</td>\n",
       "      <td>1.655736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.789300</td>\n",
       "      <td>1.490922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.715000</td>\n",
       "      <td>1.391165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.672400</td>\n",
       "      <td>1.332616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.519800</td>\n",
       "      <td>1.314015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.516600</td>\n",
       "      <td>1.327852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.434800</td>\n",
       "      <td>1.238750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.460400</td>\n",
       "      <td>1.285821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.463700</td>\n",
       "      <td>1.271576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.362800</td>\n",
       "      <td>1.241673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.387600</td>\n",
       "      <td>1.228853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.336500</td>\n",
       "      <td>1.189351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.406000</td>\n",
       "      <td>1.229232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.171700</td>\n",
       "      <td>1.231368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.163900</td>\n",
       "      <td>1.177814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.152600</td>\n",
       "      <td>1.202856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.120600</td>\n",
       "      <td>1.253620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.168400</td>\n",
       "      <td>1.218769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.177900</td>\n",
       "      <td>1.188818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.119900</td>\n",
       "      <td>1.203522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.129900</td>\n",
       "      <td>1.183292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.124800</td>\n",
       "      <td>1.190424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.128000</td>\n",
       "      <td>1.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.154500</td>\n",
       "      <td>1.194529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.113800</td>\n",
       "      <td>1.177770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.124200</td>\n",
       "      <td>1.180658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.178700</td>\n",
       "      <td>1.176153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.089600</td>\n",
       "      <td>1.153607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.939800</td>\n",
       "      <td>1.210977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.980100</td>\n",
       "      <td>1.221114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.877100</td>\n",
       "      <td>1.217518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.933000</td>\n",
       "      <td>1.228526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.968900</td>\n",
       "      <td>1.238404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.003500</td>\n",
       "      <td>1.204633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.931600</td>\n",
       "      <td>1.205125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.931300</td>\n",
       "      <td>1.219212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.976400</td>\n",
       "      <td>1.239066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.929700</td>\n",
       "      <td>1.246993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.930100</td>\n",
       "      <td>1.211887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.912900</td>\n",
       "      <td>1.199867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.922400</td>\n",
       "      <td>1.224593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.978300</td>\n",
       "      <td>1.247202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>1.246000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=450, training_loss=1.2976347965664334, metrics={'train_runtime': 2323.0681, 'train_samples_per_second': 24.781, 'train_steps_per_second': 0.194, 'total_flos': 1.1281552796265984e+16, 'train_loss': 1.2976347965664334, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.question_answering.QuestionAnsweringPipeline object at 0x75ecb7c13830>\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "checkpoint = \"output/checkpoint-450\"\n",
    "pipe = pipeline(\"question-answering\",  model=checkpoint , device=\"cuda:0\")\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.251699835062027, 'start': 3, 'end': 5, 'answer': '北京'}\n"
     ]
    }
   ],
   "source": [
    "question = \"小明在那里上学\"\n",
    "context = \"小明在北京上学\"\n",
    "print(pipe(question=question, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.3174186944961548, 'start': 4, 'end': 14, 'answer': '1979年1月18日'}\n"
     ]
    }
   ],
   "source": [
    "question = \"周杰伦什么时候出生的\"\n",
    "context = \"周杰伦（1979年1月18日—），台湾创作男歌手、演员、词曲作家及制作人。其音乐风行于大中华地区及全球各地的华人社群，并对华语乐坛产生重大影响，也是史上最具影响力及最著名的华语歌手之一[3][4][5]。\"\n",
    "print(pipe(question=question, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.47723063826560974, 'start': 774, 'end': 784, 'answer': '由于他的音乐基础扎实'}\n"
     ]
    }
   ],
   "source": [
    "question = \"为什么周杰伦在流行音乐创作方面如鱼得水\"\n",
    "context = \"\"\"\n",
    "周杰伦在台湾台北县林口乡[注 1]出生长大[12]，为家中的独生子[13][14]。父亲周耀中任教于芦洲国中，教授生物[15]；母亲叶惠美则是林口国中美术老师。14岁时父母离异，由父亲担任监护人，年满18岁后选择与母亲共同生活[16]。周杰伦曾在台湾民视新闻台由胡婉玲主持的节目《台湾演义》专访中澄清《爸，我回来了》这首歌只是对社会上家暴现象的感慨，并非指涉父母间的状况；父亲的亲戚也曾质疑过他，因此还为特别向亲戚们澄清和道歉过[17]。\n",
    "\n",
    "周杰伦自小对音乐表现出浓厚的兴趣，并且喜欢模仿歌星、演员表演和变魔术。3岁开始学习钢琴。周杰伦国小时住在台北市光华商场附近，就读忠孝国小。国中时就读金华国中[18]，此时期他的父母因长年争执而决议离婚，使周杰伦的性情大受影响。除了音乐外，周杰伦热爱篮球，在国中曾参加过篮球队。\n",
    "\n",
    "高中就读于台北县私立淡江中学第一届音乐科（本来是想报考华冈艺校，但错过了报名时间，幸好淡江中学恰巧新设了音乐科），主修钢琴，为将来的音乐发展打下了深厚的基础[19]。这时的他因正值青春期，常常秀琴技想吸引女同学的注意。但学科成绩不甚理想，故高中毕业时，大学联考落榜。又因患有僵直性脊椎炎，依据台湾兵役制度得以免服义务兵役[20]。\n",
    "\n",
    "周杰伦曾表示少年时受到香港乐坛“四大天王”之一张学友的专辑《吻别》的影响，从而喜欢并开始专注于流行音乐[21]。另外他也透露过，除了张学友以外，肖邦、李恕权与史帝夫·汪达也是他童年及成长时影响他很深的人：05年的专辑更以《十一月的肖邦》为标题，07年电影《不能说的秘密》的斗琴多处桥段和肖邦有关，示意对他致敬；李恕权每回出现在电视上，周杰伦便会在电视机面前模仿他；而史帝夫·汪达有一首《I Just Called to Say I love You》是他的婶婶曾在他叔父的葬礼中播放的歌曲。由于他的音乐基础扎实，令其在流行音乐创作方面如鱼得水。\"\"\"\n",
    "print(pipe(question=question, context=context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
